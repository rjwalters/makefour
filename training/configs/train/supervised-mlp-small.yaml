# Training configuration for supervised learning with small MLP
# Use this for better performance than tiny model

model:
  type: mlp-small
  dropout: 0.1

training:
  epochs: 200
  batch_size: 512
  learning_rate: 0.001
  weight_decay: 0.0001
  grad_clip: 1.0

  # Loss weights (slightly higher value weight for better position understanding)
  policy_weight: 1.0
  value_weight: 0.75

  # Small entropy regularization to encourage exploration
  entropy_weight: 0.01

  # Logging
  log_every: 100
  save_every: 10

  # Early stopping
  early_stopping:
    patience: 20
    metric: val_loss
    mode: min

  # Reproducibility
  seed: 42
  num_workers: 2

optimizer:
  type: adamw

scheduler:
  type: warmup_cosine
  warmup_epochs: 10
  min_lr: 0.000001

data:
  encoding: flat-binary
  augment: true
  train_ratio: 0.8
  val_ratio: 0.1
